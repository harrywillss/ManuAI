{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bc6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "from transformers import ASTForAudioClassification, ASTFeatureExtractor, TrainingArguments, ASTConfig, Trainer, EarlyStoppingCallback, AutoConfig, set_seed as t_set_seed\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio \n",
    "import torchaudio.transforms as T\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset, disable_caching, DatasetDict, Audio\n",
    "from PIL import Image, ImageOps\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d44c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed) \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    t_set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7270dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing Hugging Face datasets cache at /Users/harrywills/.cache/huggingface/datasets ...\n",
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "# Clear Hugging Face datasets cache (memory issues)\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
    "if os.path.exists(cache_dir):\n",
    "    print(f\"Clearing Hugging Face datasets cache at {cache_dir} ...\")\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cache cleared.\")\n",
    "else:\n",
    "    print(\"No Hugging Face datasets cache found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28c6ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size per class: 1000\n",
      "Using device: mps\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "training_size = int(input(\"Enter the amount of spectrograms per class to train on (0 for all): \")) # Number of spectrograms per class to use for training (0 for all)\n",
    "print(f\"Training size per class: {training_size if training_size > 0 else 'All available'}\")\n",
    "segments_path = \"./segments\"\n",
    "model_output_dir = \"./ast-base-manuai\" # Fine-tuned model output directory\n",
    "adapters_dir = \"./manuai_lora_adapters\" # LoRA adapters output directory\n",
    "checkpoints_dir = \"./manuai_checkpoints\" # Checkpoints output directory\n",
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\" # Pre-trained model name or path\n",
    "# Check if fine-tuned model already exists\n",
    "if os.path.exists(model_output_dir):\n",
    "    model_name = model_output_dir\n",
    "    print(\"Using existing fine-tuned model as base.\")\n",
    "\n",
    "processor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "sample_rate = 16000\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "n_proc = 2 # Number of processes for parallel processing\n",
    "dataloader_num_workers=0 # Number of workers for data loading (during training)\n",
    "seed = 42\n",
    "segment_len = 5.0\n",
    "lora_rank = 16\n",
    "image_size = 224  # ViT base model image size\n",
    "disable_caching() # Disable caching to avoid potential issues with large datasets\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "# tensorboard --logdir manuai_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f4081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216934c0a9534fa7ae56678d68f76d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61489 files for label 'bellbird'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1515 files for label 'fantail'\n",
      "Found 21735 files for label 'greywarbler'\n",
      "Found 1486 files for label 'kaka'\n",
      "Found 13 files for label 'kakapo'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4412 files for label 'kea'\n",
      "Found 291 files for label 'kereru'\n",
      "Found 313 files for label 'kingfisher'\n",
      "Found 11006 files for label 'kiwi'\n",
      "Found 1519 files for label 'kokako'\n",
      "Found 60698 files for label 'morepork'\n",
      "Found 1813 files for label 'pukeko'\n",
      "Found 9215 files for label 'robin'\n",
      "Found 1238 files for label 'saddleback'\n",
      "Found 50446 files for label 'silvereye'\n",
      "Found 723 files for label 'stitchbird'\n",
      "Found 58462 files for label 'tomtit'\n",
      "Found 26785 files for label 'tui'\n",
      "Found 2798 files for label 'whitehead'\n",
      "Found 773 files for label 'yellowhead'\n",
      "Total augmented samples created: 2887\n",
      "Final label order: ['bellbird', 'fantail', 'greywarbler', 'kaka', 'kakapo', 'kea', 'kereru', 'kingfisher', 'kiwi', 'kokako', 'morepork', 'pukeko', 'robin', 'saddleback', 'silvereye', 'stitchbird', 'tomtit', 'tui', 'whitehead', 'yellowhead']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8a57a2ba4455bb28a446a8b0a4a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/utils/py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3674, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3624, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3547, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/m9/5g7zcttx2sx_y5pwg05xn1dh0000gn/T/ipykernel_81860/3773121092.py\", line 14, in preprocess_audio_with_transforms\n    wavs = [audio_augmentations(audio, sample_rate=sample_rate) for audio in batch[\"input_values\"]]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/m9/5g7zcttx2sx_y5pwg05xn1dh0000gn/T/ipykernel_81860/3773121092.py\", line 14, in <listcomp>\n    wavs = [audio_augmentations(audio, sample_rate=sample_rate) for audio in batch[\"input_values\"]]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/audiomentations/core/composition.py\", line 131, in __call__\n    samples = transform(samples, sample_rate)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py\", line 106, in __call__\n    if samples.dtype == np.float64:\n       ^^^^^^^^^^^^^\nAttributeError: 'list' object has no attribute 'dtype'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal label order:\u001b[39m\u001b[33m\"\u001b[39m, labels)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Preprocess dataset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_audio_with_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_proc\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Dataset created successfully with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/arrow_dataset.py:3309\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3306\u001b[39m os.environ = prev_env\n\u001b[32m   3307\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3309\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[32m   3311\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3314\u001b[39m pool.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ManuAI/lib/python3.11/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ManuAI/lib/python3.11/site-packages/multiprocess/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "def time_stretch_waveform(waveform, rate=1.1):\n",
    "    rate = random.uniform(0.8, 1.2) if rate is None else rate\n",
    "    waveform_np = waveform.squeeze().detach().cpu().numpy()\n",
    "    if waveform_np.ndim > 1:\n",
    "        waveform_np = waveform_np.mean(axis=0)  # Convert to mono\n",
    "    stretched = librosa.effects.time_stretch(y=waveform_np, rate=rate)\n",
    "    stretched_tensor = torch.tensor(stretched, dtype=waveform.dtype, device=waveform.device)\n",
    "    if stretched_tensor.ndim == 1:\n",
    "        stretched_tensor = stretched_tensor.unsqueeze(0)  # Ensure shape [1, time]\n",
    "    return stretched_tensor\n",
    "\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(np.array(audio), sample_rate=sample_rate) for audio in batch[\"input_values\"]]\n",
    "    inputs = processor(wavs, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "    output_batch = {\"input_values\": inputs.get(\"input_values\"), \"labels\": list(batch[\"labels\"])}\n",
    "    return output_batch\n",
    "\n",
    "def augment_audio(sample, sample_rate, max_attempts=3):\n",
    "    sample = sample.to(device)\n",
    "    augmentations = [\n",
    "        lambda x: T.PitchShift(sample_rate, n_steps=random.choice([-2, -1, 1, 2])).to(device)(x), # Change pitch by -2, -1, +1, or +2 semitones\n",
    "        #lambda x: x + torch.randn_like(x) * min(0.002, x.std().item() * 0.1), # Add Gaussian noise with stddev up to 10% of original signal's stddev, capped at 0.002\n",
    "        lambda x: T.FrequencyMasking(freq_mask_param=random.randint(8, 16)).to(device)(x), # Apply frequency masking with max width of 16 bins\n",
    "        lambda x: T.TimeMasking(time_mask_param=random.randint(8, 20)).to(device)(x), # Apply time masking with max width of 20 frames\n",
    "        lambda x: time_stretch_waveform(x), # Time-stretch by a random rate between 0.8 and 1.2\n",
    "    ]\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        num_aug = random.randint(2, 3)  # Apply 2 to 3 augmentations\n",
    "        aug_funcs = random.sample(augmentations, num_aug)\n",
    "        augmented = sample.clone()  # Preserve original sample\n",
    "        \n",
    "        for augment in aug_funcs:\n",
    "            try:\n",
    "                temp_augmented = augment(augmented)\n",
    "                # Check if augmentation produces valid output\n",
    "                if is_valid_waveform(temp_augmented, min_variance=1e-8, min_amplitude=1e-4):\n",
    "                    augmented = temp_augmented\n",
    "                else:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Final validation before returning\n",
    "        if is_valid_waveform(augmented, min_variance=1e-8, min_amplitude=1e-4):\n",
    "            return augmented\n",
    "        print(f\"Attempt {attempt + 1} failed: aug_var={augmented.var().item():.6f}, aug_max={augmented.abs().max().item():.6f}\")\n",
    "    \n",
    "    # If all attempts fail, return augmented sample\n",
    "    print(\"All augmentation attempts failed, returning augmented sample\")\n",
    "    return augmented\n",
    "\n",
    "def is_valid_waveform(waveform, min_variance=1e-8, min_amplitude=1e-4):\n",
    "    # If waveform is empty or has low variance, it's invalid\n",
    "    return waveform.abs().sum() > min_amplitude and waveform.var() > min_variance\n",
    "\n",
    "def load_audio_segments():\n",
    "    \"\"\"\n",
    "    Load exactly `training_size` samples per class.\n",
    "    Uses augmentation to fill the gap if there aren't enough originals.\n",
    "    \"\"\"\n",
    "    augmented_count = 0\n",
    "    files_labels = {label: [] for label in labels}\n",
    "    for root, dirs, files in os.walk(segments_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                label = os.path.splitext(file)[0].split('_')[1]\n",
    "                files_labels[label].append(os.path.join(root, file))\n",
    "    for label in labels:\n",
    "        print(f\"Found {len(files_labels[label])} files for label '{label}'\")\n",
    "        files = files_labels[label]\n",
    "\n",
    "        # Case 1: More files than training_size -> sample down\n",
    "        if training_size > 0 and len(files) > training_size:\n",
    "            selected_files = random.sample(files, training_size)\n",
    "        else:\n",
    "            selected_files = list(files)  # copy\n",
    "        samples = []\n",
    "\n",
    "        # Load original files\n",
    "        for file_path in selected_files:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            if sr != sample_rate:\n",
    "                waveform = T.Resample(sr, sample_rate)(waveform)\n",
    "                sr = sample_rate\n",
    "\n",
    "            if not is_valid_waveform(waveform):\n",
    "                print(f\"Invalid original waveform for file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            samples.append({\n",
    "                \"input_values\": waveform.squeeze().detach().cpu().numpy(),\n",
    "                \"label\": label_to_id[label]\n",
    "            })\n",
    "\n",
    "        # Case 2: If need to augment more samples to reach training_size\n",
    "        while len(samples) < training_size and len(selected_files) > 0:\n",
    "            f = random.choice(selected_files)\n",
    "            waveform, sr = torchaudio.load(f)\n",
    "            if sr != sample_rate:\n",
    "                waveform = T.Resample(sr, sample_rate)(waveform)\n",
    "                sr = sample_rate\n",
    "\n",
    "            if not is_valid_waveform(waveform):\n",
    "                continue\n",
    "\n",
    "            augmented = augment_audio(waveform, sr)\n",
    "            if not is_valid_waveform(augmented):\n",
    "                continue\n",
    "\n",
    "            augmented_count += 1\n",
    "            samples.append({\n",
    "                \"input_values\": augmented.squeeze().detach().cpu().numpy(),\n",
    "                \"label\": label_to_id[label]\n",
    "            })\n",
    "\n",
    "        # Ensure exactly training_size (trim if overshot)\n",
    "        samples = samples[:training_size]\n",
    "\n",
    "        # Yield per-class samples\n",
    "        for s in samples:\n",
    "            yield s\n",
    "    print(f\"Total augmented samples created: {augmented_count}\")\n",
    "\n",
    "labels = sorted([d for d in os.listdir(segments_path) if not d.startswith('.')]) # Exclude hidden files\n",
    "label_to_id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id_to_label = {i: lbl for lbl, i in label_to_id.items()}\n",
    "\n",
    "dataset = Dataset.from_generator(load_audio_segments, cache_dir=None)\n",
    "\n",
    "print(\"Final label order:\", labels)\n",
    "\n",
    "# Preprocess dataset\n",
    "dataset = dataset.map(\n",
    "    preprocess_audio_with_transforms,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_values\"],\n",
    "    num_proc=n_proc\n",
    ")\n",
    "if dataset:\n",
    "    print(f\"✅ Dataset created successfully with {len(dataset)} samples.\")\n",
    "else:\n",
    "    print(\"❌ Dataset creation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_values = []\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        val = item[\"input_values\"]\n",
    "        if isinstance(val, list):\n",
    "            val = torch.tensor(val)\n",
    "        input_values.append(val)\n",
    "        labels.append(item[\"label\"])\n",
    "    input_values = torch.stack(input_values)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "config = ASTConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "base_model = ASTForAudioClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 4,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\", \n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"], # Attention layers\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b43f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, test, and validation sets\n",
    "split_labels = list(dataset[\"label\"])\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    stratify=split_labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "temp_labels = [split_labels[i] for i in temp_idx]\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Create splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset.select(train_idx),\n",
    "    \"validation\": dataset.select(val_idx),\n",
    "    \"test\": dataset.select(test_idx)\n",
    "})\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}, Test size: {len(dataset['test'])}, Validation size: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP MODEL\n",
    "def trainable_parameters(model):\n",
    "    params, trainable = 0, 0\n",
    "    for _, p in model.named_parameters():\n",
    "        params += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "\n",
    "    print(f\"{model.__class__.__name__} trainable parameters: {trainable:,}/{params:,} ({100 * trainable / params:.2f}%)\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(item[\"pixel_values\"]) for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values, \n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec = precision_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    rec = recall_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1_weighted = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    auc = roc_auc_score(labels, torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy(), multi_class='ovr', average='macro')\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"eval_f1\": f1_weighted,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "    \n",
    "def focal_loss(inputs, targets, gamma=2.0, alpha=None, weight=None, label_smoothing=0.0, reduction='mean'):\n",
    "    # per-sample CE \n",
    "    ce = F.cross_entropy(inputs, targets, weight=weight, reduction='none', label_smoothing=label_smoothing)\n",
    "    pt = torch.exp(-ce)  # p_t for the true class\n",
    "    if alpha is None:\n",
    "        alpha_t = 1.0\n",
    "    else:\n",
    "        if isinstance(alpha, (list, tuple, np.ndarray)):\n",
    "            alpha = torch.tensor(alpha, device=inputs.device, dtype=inputs.dtype)\n",
    "            alpha_t = alpha[targets]\n",
    "        elif isinstance(alpha, torch.Tensor):\n",
    "            alpha_t = alpha[targets]\n",
    "        else:\n",
    "            alpha_t = torch.tensor(float(alpha), device=inputs.device, dtype=inputs.dtype)\n",
    "\n",
    "    loss = alpha_t * ((1 - pt) ** gamma) * ce\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)   \n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=self.class_weights, label_smoothing=training_args.label_smoothing_factor) # Weighted cross-entropy loss\n",
    "        loss = loss_fn(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        #loss = focal_loss(logits.view(-1, self.model.config.num_labels), labels.view(-1), weight=self.class_weights, label_smoothing=training_args.label_smoothing_factor) # Focal loss alternative\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoints_dir,\n",
    "    logging_dir=str(checkpoints_dir + \"/runs\"),\n",
    "    learning_rate=7e-5, \n",
    "    lr_scheduler_type=\"cosine\", # \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", or \"constant_with_warmup\"\n",
    "    warmup_ratio=0.1, \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    gradient_accumulation_steps=1, # (batch_size * gradient_accumulation_steps = effective batch size)\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\", # \"steps\" or \"epoch\"\n",
    "    eval_steps=500, # Only if eval_strategy=\"steps\"\n",
    "    save_strategy=\"steps\", # \"steps\" or \"epoch\"\n",
    "    save_steps=500, # Only if save_strategy=\"steps\"\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    bf16=False,\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    label_smoothing_factor=0.05,\n",
    "    dataloader_pin_memory=False,\n",
    "    max_grad_norm=1.0,\n",
    "    use_mps_device = True if device == \"mps\" else False,\n",
    "    #dataloader_num_workers=dataloader_num_workers,\n",
    ")\n",
    "\n",
    "# Model Architecture\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=4), TensorBoardCallback()]\n",
    "\n",
    "base_model = ASTForAudioClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\", \n",
    "    target_modules=[\"query\", \"value\"], # Attention layers\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora).to(device)\n",
    "\n",
    "class_weights = torch.tensor(compute_class_weight(class_weight='balanced', classes=np.unique(dataset[\"train\"][\"label\"]), y=dataset[\"train\"][\"label\"]), dtype=torch.float32).to(device) # Compute class weights to handle class imbalance\n",
    "trainer = WeightedTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=callbacks,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "print(f\"ID to label mapping: {id_to_label}\")\n",
    "trainable_parameters(peft_model)\n",
    "print(\"Model set-up complete. Ready to begin training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a867ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "result = trainer.train()\n",
    "trainer.log_metrics(\"train\", result.metrics)\n",
    "trainer.save_metrics(\"train\", result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ON TEST SET \n",
    "metrics = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and processor\n",
    "base_model.save_pretrained(model_output_dir) # Saves full fine-tuned model (ASTForAudioClassification.from_pretrained(\"./ast-base-manuai\"))\n",
    "processor.save_pretrained(model_output_dir) # Saves feature extractor (ASTFeatureExtractor.from_pretrained(\"./ast-base-manuai\"))\n",
    "print(f\"Model fine-tuned and saved to {model_output_dir}\")\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(adapters_dir) # Saves only LoRA adapters (PEFTModel.from_pretrained(base_model, \"./manuai_lora_adapters\"))\n",
    "print(f\"LoRA adapters saved to {adapters_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ManuAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
