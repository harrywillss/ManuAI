{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92d4dc3",
   "metadata": {},
   "source": [
    "# 🎵 ManuAI Bird Call Classifier - Universal Training\n",
    "\n",
    "**Scalable preprocessing for all bird species**\n",
    "\n",
    "This streamlined notebook provides an efficient training pipeline with:\n",
    "- Essential imports and configuration\n",
    "- Universal adaptive preprocessing for all species\n",
    "- Efficient data loading and training\n",
    "- Quick evaluation and results\n",
    "- Ready for dataset expansion without species-specific dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db17d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrywills/miniconda3/envs/ManuAI/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ scikit-image available for high-quality resizing\n",
      "✅ Essential libraries imported\n",
      "🔥 PyTorch version: 2.7.1\n",
      "📱 Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Essential imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    ViTImageProcessor, ViTForImageClassification, \n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import for high-quality image resizing\n",
    "try:\n",
    "    from skimage import transform as sk_transform\n",
    "    print(\"✅ scikit-image available for high-quality resizing\")\n",
    "except ImportError:\n",
    "    sk_transform = None\n",
    "    print(\"⚠️ scikit-image not available, using PIL for resizing\")\n",
    "\n",
    "print(\"✅ Essential libraries imported\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"📱 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7002a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Configuration loaded\n",
      " Using pre-segmented data from: segments/\n",
      " Batch size: 16\n",
      " Max epochs: 20\n",
      " Learning rate: 2e-05\n",
      " Note: Each segment = 1 mel spectrogram for training\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Configuration\n",
    "config = {\n",
    "    'audio_dir': 'segments',  # Updated to use pre-segmented data\n",
    "    'dataset_splits_path': 'segment_splits.npz',  # Updated filename\n",
    "    'model_name': 'google/vit-base-patch16-224-in21k', # Pre-trained ViT model \n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 500, \n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 200,\n",
    "    'early_stopping_patience': 3,\n",
    "    'target_size': (224, 224),\n",
    "    'validate_quality': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "\n",
    "print(\"⚙️ Configuration loaded\")\n",
    "print(f\" Using pre-segmented data from: {config['audio_dir']}/\")\n",
    "print(f\" Batch size: {config['batch_size']}\")\n",
    "print(f\" Max epochs: {config['num_epochs']}\")\n",
    "print(f\" Learning rate: {config['learning_rate']}\")\n",
    "print(\" Note: Each segment = 1 mel spectrogram for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3507f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Dataset splitting configuration:\n",
      "   Force new splits: False\n",
      "   Test size: 20%\n",
      "   Validation size: 20%\n",
      "   Train size: 60%\n",
      "   💡 Set 'force_new_splits=True' to regenerate splits\n",
      "   🎛️ Use dataset_size_config above to control dataset size and balancing\n"
     ]
    }
   ],
   "source": [
    "# Dataset Splitting Configuration\n",
    "splitting_config = {\n",
    "    'force_new_splits': False,  # Set to True to always create new splits\n",
    "    'test_size': 0.2,          # 20% for test set\n",
    "    'val_size': 0.2,           # 20% for validation set (60% train, 20% val, 20% test)\n",
    "    'stratify': True,          # Ensure balanced splits across species\n",
    "    'show_split_info': True    # Display detailed split information\n",
    "}\n",
    "\n",
    "print(\"⚙️ Dataset splitting configuration:\")\n",
    "print(f\"   Force new splits: {splitting_config['force_new_splits']}\")\n",
    "print(f\"   Test size: {splitting_config['test_size']*100:.0f}%\")\n",
    "print(f\"   Validation size: {splitting_config['val_size']*100:.0f}%\")\n",
    "print(f\"   Train size: {(1-splitting_config['test_size']-splitting_config['val_size'])*100:.0f}%\")\n",
    "print(\"   💡 Set 'force_new_splits=True' to regenerate splits\")\n",
    "print(\"   🎛️ Use dataset_size_config above to control dataset size and balancing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5669157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UniversalAudioDataset class ready - scalable for all species\n"
     ]
    }
   ],
   "source": [
    "# Universal Dataset Class with Adaptive Preprocessing\n",
    "class UniversalAudioDataset(Dataset):\n",
    "    \"\"\"Dataset with robust universal preprocessing for all bird species.\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_paths, labels, label_encoder, transform=None, target_size=(224, 224)):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Initialize ViT processor\n",
    "        self.processor = ViTImageProcessor.from_pretrained(\n",
    "            'google/vit-base-patch16-224-in21k'\n",
    "        )\n",
    "    \n",
    "    def adaptive_preprocessing(self, audio, sr):\n",
    "        \"\"\"Adaptive preprocessing based on audio characteristics.\"\"\"\n",
    "        # Normalize loudness\n",
    "        if np.max(np.abs(audio)) > 0:\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "        \n",
    "        # Adaptive parameters based on signal characteristics\n",
    "        rms_energy = np.sqrt(np.mean(audio**2))\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
    "        \n",
    "        # Adaptive mel parameters\n",
    "        if rms_energy < 0.1:  # Low energy signal\n",
    "            n_mels = 128\n",
    "            hop_length = 256  \n",
    "        else:  # Normal signal\n",
    "            n_mels = 128\n",
    "            hop_length = 256\n",
    "            \n",
    "        # Frequency adaptation\n",
    "        if spectral_centroid > 4000:  # High frequency signal\n",
    "            fmax = 10000\n",
    "        else:  # Normal range\n",
    "            fmax = 8000\n",
    "            \n",
    "        return n_mels, hop_length, fmax\n",
    "    \n",
    "    def create_robust_spectrogram(self, audio, sr):\n",
    "        \"\"\"Create robust mel spectrogram with universal parameters.\"\"\"\n",
    "        try:\n",
    "            # Get adaptive parameters\n",
    "            n_mels, hop_length, fmax = self.adaptive_preprocessing(audio, sr)\n",
    "            \n",
    "            # Generate mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio, \n",
    "                sr=sr, \n",
    "                n_mels=n_mels,\n",
    "                hop_length=hop_length,\n",
    "                win_length=1024,\n",
    "                fmax=fmax\n",
    "            )\n",
    "            \n",
    "            # Convert to dB and resize\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Resize to target dimensions\n",
    "            if mel_spec_db.shape != self.target_size:\n",
    "                # Adjust height (mel bins)\n",
    "                if mel_spec_db.shape[0] != self.target_size[0]:\n",
    "                    if mel_spec_db.shape[0] < self.target_size[0]:\n",
    "                        mel_spec_db = np.pad(mel_spec_db, \n",
    "                                           ((0, self.target_size[0] - mel_spec_db.shape[0]), (0, 0)), \n",
    "                                           mode='constant')\n",
    "                    else:\n",
    "                        mel_spec_db = mel_spec_db[:self.target_size[0], :]\n",
    "                \n",
    "                # Adjust width (time)\n",
    "                if mel_spec_db.shape[1] != self.target_size[1]:\n",
    "                    if mel_spec_db.shape[1] < self.target_size[1]:\n",
    "                        mel_spec_db = np.pad(mel_spec_db,\n",
    "                                           ((0, 0), (0, self.target_size[1] - mel_spec_db.shape[1])),\n",
    "                                           mode='constant')\n",
    "                    else:\n",
    "                        mel_spec_db = mel_spec_db[:, :self.target_size[1]]\n",
    "            \n",
    "            # Normalize to 0-255\n",
    "            mel_spec_normalized = ((mel_spec_db - mel_spec_db.min()) / \n",
    "                                 (mel_spec_db.max() - mel_spec_db.min() + 1e-8) * 255).astype(np.uint8)\n",
    "            \n",
    "            # Convert to RGB PIL Image\n",
    "            spectrogram_image = Image.fromarray(mel_spec_normalized, mode='L').convert('RGB')\n",
    "            \n",
    "            return spectrogram_image\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in spectrogram creation: {e}\")\n",
    "            # Return black image as fallback\n",
    "            return Image.new('RGB', self.target_size, color='black')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Create spectrogram \n",
    "            spectrogram_image = self.create_robust_spectrogram(audio, sr)\n",
    "            \n",
    "            # Apply transforms if provided\n",
    "            if self.transform:\n",
    "                spectrogram_image = self.transform(spectrogram_image)\n",
    "                # Convert back to PIL if transform returned tensor\n",
    "                if isinstance(spectrogram_image, torch.Tensor):\n",
    "                    spectrogram_image = transforms.ToPILImage()(spectrogram_image)\n",
    "            \n",
    "            # Process for ViT\n",
    "            inputs = self.processor(images=spectrogram_image, return_tensors=\"pt\")\n",
    "            \n",
    "            return {\n",
    "                'pixel_values': inputs['pixel_values'].squeeze(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            # Robust fallback\n",
    "            black_image = Image.new('RGB', self.target_size, color='black')\n",
    "            inputs = self.processor(images=black_image, return_tensors=\"pt\")\n",
    "            return {\n",
    "                'pixel_values': inputs['pixel_values'].squeeze(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "print(\"✅ UniversalAudioDataset class ready - scalable for all species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27b2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading/creating segment-based dataset splits with size control...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_size_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m segments_directory = \u001b[33m'\u001b[39m\u001b[33msegments\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create a unique splits filename based on configuration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m size_suffix = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataset_size_config\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset_size_config[\u001b[33m'\u001b[39m\u001b[33mbalance_classes\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     10\u001b[39m     size_suffix += \u001b[33m\"\u001b[39m\u001b[33m_balanced\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_size_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Load or create segment-based dataset splits with size control\n",
    "print(\"📂 Loading/creating segment-based dataset splits with size control...\")\n",
    "\n",
    "# Use segments directory - this contains the actual training data (pre-segmented)\n",
    "segments_directory = 'segments'\n",
    "\n",
    "# Create a unique splits filename based on configuration\n",
    "size_suffix = f\"_{dataset_size_config['size']}\"\n",
    "if dataset_size_config['balance_classes']:\n",
    "    size_suffix += \"_balanced\"\n",
    "splits_filename = f\"segment_splits{size_suffix}.npz\"\n",
    "controlled_splits_path = splits_filename\n",
    "\n",
    "# Determine if we need to create new splits\n",
    "splits_exist = os.path.exists(controlled_splits_path)\n",
    "create_new_splits = splitting_config['force_new_splits'] or not splits_exist\n",
    "\n",
    "if splits_exist and not splitting_config['force_new_splits']:\n",
    "    print(f\"✅ Found existing segment splits file: {controlled_splits_path}\")\n",
    "    print(\"💡 Set splitting_config['force_new_splits']=True to regenerate\")\n",
    "elif splits_exist and splitting_config['force_new_splits']:\n",
    "    print(f\"🔄 Forcing creation of new splits (existing file will be overwritten)\")\n",
    "else:\n",
    "    print(f\"❌ No splits file found at {controlled_splits_path}\")\n",
    "    print(\"🆕 Creating new segment splits automatically\")\n",
    "\n",
    "# Load existing or create new splits\n",
    "if create_new_splits:\n",
    "    print(f\"\\n🎛️ Creating {dataset_size_config['size']} segment dataset with selected species...\")\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = create_controlled_segment_splits(\n",
    "        segments_directory, \n",
    "        selected_species_limits,\n",
    "        test_size=splitting_config['test_size'], \n",
    "        val_size=splitting_config['val_size'], \n",
    "        seed=config['seed'],\n",
    "        show_info=splitting_config['show_split_info']\n",
    "    )\n",
    "    \n",
    "    # Save the splits for future use\n",
    "    np.savez(\n",
    "        controlled_splits_path,\n",
    "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "        # Save configuration for reference\n",
    "        size_config=dataset_size_config['size'],\n",
    "        species_limits=selected_species_limits,\n",
    "        data_type='segments'  # Flag to indicate this uses segments\n",
    "    )\n",
    "    print(f\"💾 Controlled segment splits saved to {controlled_splits_path}\")\n",
    "    \n",
    "else:\n",
    "    # Load existing splits\n",
    "    data_splits = np.load(controlled_splits_path, allow_pickle=True)\n",
    "    X_train, X_val, X_test = data_splits['X_train'], data_splits['X_val'], data_splits['X_test']\n",
    "    y_train, y_val, y_test = data_splits['y_train'], data_splits['y_val'], data_splits['y_test']\n",
    "    \n",
    "    # Try to load configuration info if available\n",
    "    try:\n",
    "        saved_size_config = data_splits['size_config'].item() if 'size_config' in data_splits else 'unknown'\n",
    "        saved_species_limits = data_splits['species_limits'].item() if 'species_limits' in data_splits else {}\n",
    "        data_type = data_splits['data_type'].item() if 'data_type' in data_splits else 'unknown'\n",
    "        print(f\"📁 Loaded existing {saved_size_config} {data_type} dataset splits:\")\n",
    "    except:\n",
    "        print(f\"📁 Loaded existing splits:\")\n",
    "    \n",
    "    if splitting_config['show_split_info']:\n",
    "        print(f\"   Train: {len(X_train)} spectrograms\")\n",
    "        print(f\"   Validation: {len(X_val)} spectrograms\") \n",
    "        print(f\"   Test: {len(X_test)} spectrograms\")\n",
    "\n",
    "# Create label encoder\n",
    "all_labels = np.concatenate([y_train, y_val, y_test])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Convert to numeric labels\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"\\n✅ {dataset_size_config['size'].title()} segment dataset ready:\")\n",
    "print(f\"   Train: {len(X_train)} spectrograms\")\n",
    "print(f\"   Validation: {len(X_val)} spectrograms\")\n",
    "print(f\"   Test: {len(X_test)} spectrograms\")\n",
    "print(f\"   Classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"   Species: {', '.join(map(str, label_encoder.classes_))}\")\n",
    "\n",
    "# Enhanced data transforms for better generalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-3, 3)),  # Slight temporal shifts\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1),  # Spectral variations\n",
    "    transforms.RandomHorizontalFlip(p=0.1),  # Rare but helps with temporal reversals\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))\n",
    "    ], p=0.1),  # Slight smoothing to simulate recording conditions\n",
    "])\n",
    "\n",
    "transform_val = None  # No augmentation for validation\n",
    "\n",
    "print(\"✅ Enhanced universal data transforms ready\")\n",
    "print(\"🎵 Note: Using pre-segmented audio files - each segment becomes 1 mel spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc344396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze actual segmented dataset (the real training data)\n",
    "print(\"🔍 Analyzing segmented dataset (actual training data)...\")\n",
    "\n",
    "# Update to use segments directory - this is the real training data!\n",
    "segments_dir = 'segments'\n",
    "\n",
    "def analyze_segmented_dataset(segments_dir):\n",
    "    \"\"\"Analyze the actual segmented dataset that will be used for training.\"\"\"\n",
    "    print(f\"📊 Analyzing segmented dataset in {segments_dir}...\")\n",
    "    \n",
    "    # Get all segment files from species folders\n",
    "    segment_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for species_folder in os.listdir(segments_dir):\n",
    "        species_path = os.path.join(segments_dir, species_folder)\n",
    "        if os.path.isdir(species_path):\n",
    "            # Find all .wav files in this species folder (including subdirectories)\n",
    "            species_files = glob.glob(os.path.join(species_path, '**', '*.wav'), recursive=True)\n",
    "            \n",
    "            segment_paths.extend(species_files)\n",
    "            labels.extend([species_folder] * len(species_files))\n",
    "            \n",
    "            print(f\"   {species_folder}: {len(species_files)} segments\")\n",
    "    \n",
    "    # Analyze distribution\n",
    "    species_counts = Counter(labels)\n",
    "    total_segments = len(segment_paths)\n",
    "    \n",
    "    print(f\"\\n📈 Segmented Dataset Distribution ({total_segments} total segments):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    sorted_species = sorted(species_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    min_segments = min(species_counts.values())\n",
    "    max_segments = max(species_counts.values())\n",
    "    \n",
    "    for species, count in sorted_species:\n",
    "        percentage = (count / total_segments) * 100\n",
    "        print(f\"   {species.ljust(12)}: {count:5d} segments ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 Segment Distribution Summary:\")\n",
    "    print(f\"   • Minimum segments per class: {min_segments}\")\n",
    "    print(f\"   • Maximum segments per class: {max_segments}\")\n",
    "    print(f\"   • Imbalance ratio: {max_segments/min_segments:.2f}:1\")\n",
    "    \n",
    "    # Determine if dataset is balanced\n",
    "    is_balanced = (max_segments / min_segments) <= 2.0  # Allow up to 2:1 ratio as \"balanced\"\n",
    "    balance_status = \"✅ Relatively balanced\" if is_balanced else \"⚠️ Highly imbalanced\"\n",
    "    print(f\"   • Balance status: {balance_status}\")\n",
    "    \n",
    "    # Calculate expected spectrograms (1:1 ratio with segments for individual processing)\n",
    "    print(f\"\\n🎵 Spectrogram Count (1 per segment):\")\n",
    "    print(f\"   • Total spectrograms for training: {total_segments}\")\n",
    "    print(f\"   • Each segment → 1 mel spectrogram\")\n",
    "    \n",
    "    return species_counts, sorted_species, segment_paths, labels\n",
    "\n",
    "# Analyze actual segmented dataset\n",
    "species_counts, sorted_species, all_segment_paths, all_segment_labels = analyze_segmented_dataset(segments_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-Based Dataset Size Selection Configuration\n",
    "dataset_size_config = {\n",
    "    'size': 'medium',  # Options: 'small', 'medium', 'large', 'full'\n",
    "    'balance_classes': True,  # Whether to balance classes by limiting to min class size\n",
    "    'min_segments_per_class': 50,  # Minimum segments required per class\n",
    "    'show_size_info': True\n",
    "}\n",
    "\n",
    "def create_sized_segment_dataset(species_counts, size='medium', balance_classes=True, min_segments_per_class=50):\n",
    "    \"\"\"\n",
    "    Create different sized datasets based on segment counts (actual spectrograms).\n",
    "    All sizes include ALL classes - only the training data amount per class varies.\n",
    "    \n",
    "    Args:\n",
    "        species_counts: Counter object with species and their segment counts\n",
    "        size: 'small', 'medium', 'large', or 'full'\n",
    "        balance_classes: If True, limit all classes to the size of the smallest class\n",
    "        min_segments_per_class: Minimum segments required per class\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🎛️ Creating '{size}' segment-based dataset...\")\n",
    "    \n",
    "    # Define size parameters based on spectrogram counts\n",
    "    # All sizes use ALL CLASSES - only training data amount varies\n",
    "    size_configs = {\n",
    "        'small': {\n",
    "            'max_segments_per_class': 200,\n",
    "            'description': 'Quick prototyping (200 spectrograms/class, all classes)',\n",
    "            'estimated_time': '5-15 minutes',\n",
    "            'total_estimate': 2000  # 200 * 10 classes\n",
    "        },\n",
    "        'medium': {\n",
    "            'max_segments_per_class': 800,\n",
    "            'description': 'Balanced experimentation (800 spectrograms/class, all classes)',\n",
    "            'estimated_time': '30-60 minutes',\n",
    "            'total_estimate': 8000  # 800 * 10 classes\n",
    "        },\n",
    "        'large': {\n",
    "            'max_segments_per_class': 1500,\n",
    "            'description': 'Comprehensive training (1500 spectrograms/class, all classes)',\n",
    "            'estimated_time': '2-4 hours',\n",
    "            'total_estimate': 15000  # 1500 * 10 classes\n",
    "        },\n",
    "        'full': {\n",
    "            'max_segments_per_class': float('inf'),\n",
    "            'description': 'Complete dataset (all available spectrograms, all classes)',\n",
    "            'estimated_time': '4-8 hours',\n",
    "            'total_estimate': 28706\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if size not in size_configs:\n",
    "        raise ValueError(f\"Size must be one of: {list(size_configs.keys())}\")\n",
    "    \n",
    "    config = size_configs[size]\n",
    "    print(f\"📝 Configuration: {config['description']}\")\n",
    "    print(f\"⏱️ Estimated training time: {config['estimated_time']}\")\n",
    "    \n",
    "    # Use ALL species - sort by segment count for consistency\n",
    "    sorted_species = sorted(species_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"🎯 Using all {len(sorted_species)} classes for consistent model architecture\")\n",
    "    \n",
    "    # Determine segments per class\n",
    "    species_limits = {}\n",
    "    \n",
    "    if balance_classes and size != 'full':\n",
    "        # For balanced datasets, find the limiting factor\n",
    "        available_segments = [count for species, count in sorted_species]\n",
    "        \n",
    "        if size in ['small', 'medium', 'large']:\n",
    "            # Use the configured max, but don't exceed what's available\n",
    "            # Also consider the minimum available if balancing\n",
    "            if balance_classes:\n",
    "                limit = min(config['max_segments_per_class'], min(available_segments))\n",
    "            else:\n",
    "                limit = config['max_segments_per_class']\n",
    "        else:\n",
    "            limit = min(available_segments)\n",
    "        \n",
    "        print(f\"⚖️ Balancing classes: {limit} segments per class\")\n",
    "        for species, _ in sorted_species:\n",
    "            species_limits[species] = limit\n",
    "            \n",
    "    else:\n",
    "        # Use available segments up to the configured maximum\n",
    "        print(f\"📊 Using available segments (max {config['max_segments_per_class']} per class)\")\n",
    "        for species, count in sorted_species:\n",
    "            if size == 'full':\n",
    "                species_limits[species] = count\n",
    "            else:\n",
    "                species_limits[species] = min(count, config['max_segments_per_class'])\n",
    "    \n",
    "    # Verify minimum requirements and handle classes with insufficient data\n",
    "    valid_species = {}\n",
    "    insufficient_species = []\n",
    "    \n",
    "    for species, limit in species_limits.items():\n",
    "        if limit >= min_segments_per_class:\n",
    "            valid_species[species] = limit\n",
    "        else:\n",
    "            # For classes with insufficient data, use what's available but warn\n",
    "            available = species_counts[species]\n",
    "            valid_species[species] = available\n",
    "            insufficient_species.append((species, available))\n",
    "            print(f\"⚠️ {species}: only {available} segments available (< {min_segments_per_class} desired)\")\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_segments = sum(valid_species.values())\n",
    "    \n",
    "    print(f\"\\n📈 Segment Dataset Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    for species, limit in sorted(valid_species.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (limit / total_segments) * 100\n",
    "        available = species_counts[species]\n",
    "        status = \"⚠️\" if limit < min_segments_per_class else \"✅\"\n",
    "        print(f\"   {status} {species.ljust(12)}: {limit:4d} segments ({percentage:5.1f}%) [of {available} available]\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total: {total_segments} segments across {len(valid_species)} classes\")\n",
    "    print(f\"   🎵 = {total_segments} mel spectrograms for training\")\n",
    "    \n",
    "    # Calculate approximate train/val/test split sizes\n",
    "    train_size = int(total_segments * 0.6)\n",
    "    val_size = int(total_segments * 0.2)\n",
    "    test_size = total_segments - train_size - val_size\n",
    "    \n",
    "    print(f\"\\n🔄 Approximate split sizes:\")\n",
    "    print(f\"   Train: ~{train_size} spectrograms\")\n",
    "    print(f\"   Validation: ~{val_size} spectrograms\")\n",
    "    print(f\"   Test: ~{test_size} spectrograms\")\n",
    "    \n",
    "    # Balance efficiency info\n",
    "    if balance_classes:\n",
    "        original_total = sum(species_counts[species] for species in valid_species.keys())\n",
    "        efficiency = (total_segments / original_total) * 100\n",
    "        print(f\"\\n📊 Dataset efficiency: {efficiency:.1f}% of available segments used\")\n",
    "        if efficiency < 50:\n",
    "            print(\"   💡 Consider 'balance_classes=False' to use more data\")\n",
    "    \n",
    "    # Model consistency info\n",
    "    print(f\"\\n🏗️ Model Architecture:\")\n",
    "    print(f\"   Classes: {len(valid_species)} (same across all dataset sizes)\")\n",
    "    print(f\"   Species: {list(valid_species.keys())}\")\n",
    "    if insufficient_species:\n",
    "        print(f\"   Note: {len(insufficient_species)} classes have limited data\")\n",
    "    \n",
    "    return valid_species, config\n",
    "\n",
    "# Create the configured segment dataset size\n",
    "selected_species_limits, size_config = create_sized_segment_dataset(\n",
    "    species_counts, \n",
    "    size=dataset_size_config['size'],\n",
    "    balance_classes=dataset_size_config['balance_classes'],\n",
    "    min_segments_per_class=dataset_size_config['min_segments_per_class']\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Segment-based dataset configuration complete!\")\n",
    "print(f\"💡 To change size, modify dataset_size_config['size'] to: 'small', 'medium', 'large', or 'full'\")\n",
    "print(f\"🔧 Toggle balancing with dataset_size_config['balance_classes'] = True/False\")\n",
    "print(f\"🎯 All dataset sizes use the same {len(selected_species_limits)} classes for consistent comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ca46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Segment-Based Dataset Creation with Size Control\n",
    "def create_controlled_segment_splits(segments_dir, species_limits, test_size=0.2, val_size=0.2, seed=42, show_info=True):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits with controlled species and segment limits.\n",
    "    \n",
    "    Args:\n",
    "        segments_dir: Directory containing segmented audio files organized by species\n",
    "        species_limits: Dict mapping species names to maximum segment counts\n",
    "        test_size, val_size: Split proportions\n",
    "        seed: Random seed for reproducibility\n",
    "        show_info: Whether to display detailed information\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Creating controlled segment-based dataset splits...\")\n",
    "    \n",
    "    # Get all segment files organized by species\n",
    "    species_segments = {}\n",
    "    total_available = 0\n",
    "    \n",
    "    for species in species_limits.keys():\n",
    "        species_path = os.path.join(segments_dir, species)\n",
    "        if os.path.isdir(species_path):\n",
    "            # Find all .wav files in this species folder (including subdirectories)\n",
    "            species_files = glob.glob(os.path.join(species_path, '**', '*.wav'), recursive=True)\n",
    "            species_segments[species] = species_files\n",
    "            total_available += len(species_files)\n",
    "            \n",
    "            if show_info:\n",
    "                print(f\"   Found {len(species_files)} {species} segments\")\n",
    "    \n",
    "    # Apply segment limits and shuffle\n",
    "    np.random.seed(seed)\n",
    "    controlled_paths = []\n",
    "    controlled_labels = []\n",
    "    \n",
    "    total_selected = 0\n",
    "    for species, limit in species_limits.items():\n",
    "        if species in species_segments:\n",
    "            files = species_segments[species]\n",
    "            # Shuffle and take up to the limit\n",
    "            np.random.shuffle(files)\n",
    "            selected_files = files[:limit]\n",
    "            \n",
    "            for file_path in selected_files:\n",
    "                controlled_paths.append(file_path)\n",
    "                controlled_labels.append(species)\n",
    "                total_selected += 1\n",
    "    \n",
    "    if show_info:\n",
    "        unique_species = sorted(set(controlled_labels))\n",
    "        print(f\"\\n✅ Segment selection complete:\")\n",
    "        print(f\"   Selected: {total_selected} segments from {len(unique_species)} species\")\n",
    "        print(f\"   Species: {sorted(unique_species)} ({len(unique_species)} total)\")\n",
    "        efficiency = (total_selected / total_available) * 100 if total_available > 0 else 0\n",
    "        print(f\"   📈 Dataset efficiency: {efficiency:.1f}% of available segments used\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X = np.array(controlled_paths)\n",
    "    y = np.array(controlled_labels)\n",
    "    \n",
    "    # Create splits with stratification\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "    \n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, random_state=seed, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    if show_info:\n",
    "        print(f\"\\n✅ Controlled segment splits created:\")\n",
    "        print(f\"   Train: {len(X_train)} spectrograms ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "        print(f\"   Validation: {len(X_val)} spectrograms ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "        print(f\"   Test: {len(X_test)} spectrograms ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        # Show balance in each split\n",
    "        print(f\"\\n⚖️ Class balance verification:\")\n",
    "        for split_name, split_labels in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "            species_in_split = Counter(split_labels)\n",
    "            min_samples = min(species_in_split.values()) if species_in_split else 0\n",
    "            max_samples = max(species_in_split.values()) if species_in_split else 0\n",
    "            balance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "            print(f\"   {split_name}: {min_samples}-{max_samples} spectrograms/class (ratio: {balance_ratio:.2f}:1)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "print(\"✅ Controlled segment-based dataset creation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎛️ Segment-Based Dataset Size Options Demo\n",
    "print(\"🎛️ Available Segment-Based Dataset Size Options:\")\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 All sizes use the SAME number of classes - only training data amount varies\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Demo all size options with segment counts\n",
    "demo_sizes = ['small', 'medium', 'large', 'full']\n",
    "\n",
    "for size in demo_sizes:\n",
    "    print(f\"\\n📊 {size.upper()} Segment Dataset:\")\n",
    "    demo_limits, demo_config = create_sized_segment_dataset(\n",
    "        species_counts, \n",
    "        size=size,\n",
    "        balance_classes=True,  # Show balanced version\n",
    "        min_segments_per_class=50\n",
    "    )\n",
    "    \n",
    "    total_segments = sum(demo_limits.values())\n",
    "    train_approx = int(total_segments * 0.6)\n",
    "    val_approx = int(total_segments * 0.2)\n",
    "    test_approx = total_segments - train_approx - val_approx\n",
    "    \n",
    "    print(f\"   📈 Training time estimate: {demo_config['estimated_time']}\")\n",
    "    print(f\"   🎵 Total spectrograms: {total_segments}\")\n",
    "    print(f\"   🏗️ Classes: {len(demo_limits)} (consistent across all sizes)\")\n",
    "    if size == 'small':\n",
    "        print(\"      🎯 Perfect for: Rapid prototyping, algorithm testing\")\n",
    "        print(\"      💡 Best for: Initial experiments and debugging\")\n",
    "    elif size == 'medium':\n",
    "        print(\"      🎯 Perfect for: Hyperparameter tuning, model comparison\")\n",
    "        print(\"      💡 Best for: Finding optimal settings\")\n",
    "    elif size == 'large':\n",
    "        print(\"      🎯 Perfect for: Comprehensive evaluation, final training\")\n",
    "        print(\"      💡 Best for: Thorough model assessment\")\n",
    "    elif size == 'full':\n",
    "        print(\"      🎯 Perfect for: Production model, maximum performance\")\n",
    "        print(\"      💡 Best for: Final deployment model\")\n",
    "\n",
    "print(\"\\n\"+\"=\"*70)\n",
    "print(\"🚀 Recommended Development Workflow:\")\n",
    "print(\"   1. 'small' → Rapid prototyping & debugging (~2K spectrograms, 10 classes)\")\n",
    "print(\"   2. 'medium' → Hyperparameter optimization (~8K spectrograms, 10 classes)\")\n",
    "print(\"   3. 'large' → Comprehensive evaluation (~15K spectrograms, 10 classes)\")\n",
    "print(\"   4. 'full' → Production training (~29K spectrograms, 10 classes)\")\n",
    "print(\"\\n🎯 Key Benefits:\")\n",
    "print(\"   ✅ Consistent model architecture across all experiments\")\n",
    "print(\"   ✅ Fair comparison between different dataset sizes\")\n",
    "print(\"   ✅ No need to retrain with different class counts\")\n",
    "print(\"   ✅ Same evaluation metrics across all experiments\")\n",
    "print(\"\\n🔧 To change size: modify dataset_size_config['size'] above and re-run dataset loading\")\n",
    "print(\"⚖️ Toggle balancing: dataset_size_config['balance_classes'] = True/False\")\n",
    "print(\"🎵 Note: Each segment = 1 mel spectrogram for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dec3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Quick Test: Different Dataset Size Configurations\n",
    "print(\"🧪 Testing Different Dataset Size Configurations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test different dataset sizes to show consistent class counts\n",
    "test_sizes = ['small', 'medium', 'large', 'full']\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    print(f\"\\n📊 {test_size.upper()} Dataset Configuration:\")\n",
    "    test_limits, test_config = create_sized_segment_dataset(\n",
    "        species_counts, \n",
    "        size=test_size,\n",
    "        balance_classes=True,\n",
    "        min_segments_per_class=50\n",
    "    )\n",
    "    total_spectrograms = sum(test_limits.values())\n",
    "    min_per_class = min(test_limits.values()) if test_limits else 0\n",
    "    max_per_class = max(test_limits.values()) if test_limits else 0\n",
    "    \n",
    "    print(f\"   Classes: {len(test_limits)} (same for all sizes)\")\n",
    "    print(f\"   Spectrograms per class: {min_per_class}-{max_per_class}\")\n",
    "    print(f\"   Total spectrograms: {total_spectrograms}\")\n",
    "    print(f\"   Estimated time: {test_config['estimated_time']}\")\n",
    "\n",
    "print(f\"\\n🔄 Current setting: {dataset_size_config['size']} dataset\")\n",
    "print(f\"📊 Current classes: {len(selected_species_limits)} (consistent across all sizes)\")\n",
    "print(f\"📈 Current total spectrograms: {sum(selected_species_limits.values())}\")\n",
    "\n",
    "print(f\"\\n💡 To switch dataset size:\")\n",
    "print(\"   1. Change: dataset_size_config['size'] = 'small'/'medium'/'large'/'full'\")\n",
    "print(\"   2. Set: splitting_config['force_new_splits'] = True\")\n",
    "print(\"   3. Re-run the dataset loading cells\")\n",
    "print(f\"   4. Result: Always 10 classes, different training data amounts\")\n",
    "\n",
    "print(f\"\\n🎯 Key Benefits:\")\n",
    "print(\"   ✅ Same model architecture for all experiments\")\n",
    "print(\"   ✅ Fair comparison between dataset sizes\")\n",
    "print(\"   ✅ Consistent evaluation metrics\")\n",
    "print(\"   ✅ No need to change model configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create universal datasets\n",
    "print(\"🔧 Creating universal datasets...\")\n",
    "\n",
    "train_dataset = UniversalAudioDataset(\n",
    "    X_train, y_train_encoded, label_encoder, \n",
    "    transform=transform_train, target_size=config['target_size']\n",
    ")\n",
    "\n",
    "val_dataset = UniversalAudioDataset(\n",
    "    X_val, y_val_encoded, label_encoder, \n",
    "    transform=transform_val, target_size=config['target_size']\n",
    ")\n",
    "\n",
    "test_dataset = UniversalAudioDataset(\n",
    "    X_test, y_test_encoded, label_encoder, \n",
    "    transform=transform_val, target_size=config['target_size']\n",
    ")\n",
    "\n",
    "print(\"✅ Datasets created with universal adaptive preprocessing\")\n",
    "\n",
    "# Quick test\n",
    "sample = train_dataset[0]\n",
    "print(f\"📊 Sample check: {sample['pixel_values'].shape}, label: {label_encoder.classes_[sample['labels'].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup and training\n",
    "print(\"🤖 Setting up model...\")\n",
    "\n",
    "# Load model and processor\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    config['model_name'], \n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "processor = ViTImageProcessor.from_pretrained(config['model_name'])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-base-manuai',\n",
    "    num_train_epochs=config['num_epochs'],\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    per_device_eval_batch_size=config['batch_size'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_steps=config['eval_steps'],\n",
    "    save_steps=config['save_steps'],\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    ")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Data collator\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=config['early_stopping_patience'])\n",
    "\n",
    "print(f\"✅ Model loaded: {config['model_name']}\")\n",
    "print(f\"🎯 Output classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"⚙️ Training configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b38110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a quick overview of the training setup\n",
    "print(\"\\n🔍 Training setup:\")\n",
    "print(f\"   Model: {config['model_name']}\")\n",
    "print(f\"   Epochs: {config['num_epochs']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Warmup steps: {config['warmup_steps']}\")\n",
    "print(f\"   Evaluation steps: {config['eval_steps']}\")\n",
    "print(f\"   Save steps: {config['save_steps']}\")\n",
    "print(f\"   Early stopping patience: {config['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"🚀 Starting training...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_results = trainer.train()\n",
    "\n",
    "print(\"✅ Training completed!\")\n",
    "print(f\"📈 Final train loss: {train_results.training_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"💾 Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"📊 Evaluating model performance...\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"\\n🎯 Validation Results:\")\n",
    "print(f\"   Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {val_results['eval_f1']:.4f}\")\n",
    "print(f\"   Precision: {val_results['eval_precision']:.4f}\")\n",
    "print(f\"   Recall: {val_results['eval_recall']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"\\n🧪 Test Results:\")\n",
    "print(f\"   Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"   Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"   Recall: {test_results['eval_recall']:.4f}\")\n",
    "\n",
    "# Detailed per-class analysis\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n📋 Detailed Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_, digits=3))\n",
    "\n",
    "# Focus on challenging species performance\n",
    "print(\"\\n🎯 Species Performance Overview:\")\n",
    "for species in label_encoder.classes_:\n",
    "    species_idx = np.where(label_encoder.classes_ == species)[0][0]\n",
    "    species_mask = y_true == species_idx\n",
    "    if np.sum(species_mask) > 0:  # Only if species has test samples\n",
    "        species_accuracy = np.mean(y_pred[species_mask] == y_true[species_mask])\n",
    "        print(f\"   {species.title()}: {species_accuracy:.3f} ({species_accuracy*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Training and evaluation complete!\")\n",
    "print(\"\\n🎯 Quick Summary:\")\n",
    "print(\"   • Universal adaptive preprocessing that scales with dataset expansion\")\n",
    "print(\"   • Robust preprocessing based on audio characteristics, not species names\")\n",
    "print(\"   • Enhanced data augmentation for better generalization\")\n",
    "print(\"   • Ready for production use with any number of bird species\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ManuAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
