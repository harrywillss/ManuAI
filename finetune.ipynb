{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebd164b",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning of ViT Model for NZ Bird Sound Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe72d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\n",
    "import os\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import defaultdict\n",
    "# Calculate class weights to handle imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "segments_dir = \"./segments\" # Directory containing the segmented audio files (segments/{bird_name}/{recording_name}.wav)\n",
    "output_dir = \"./vit-base-manuai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53fff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Directory structure:\n",
    "# segments/\n",
    "#   â”œâ”€â”€ tui/\n",
    "#   â”œâ”€â”€ bellbird/\n",
    "#   â”œâ”€â”€ kaka/\n",
    "    #   â”œâ”€â”€ {scientific_name}_{scientific_subspecies}/\n",
    "    #   â”‚   â”œâ”€â”€ {id}_{english_name}_{scientific_name}_{scientific_subspecies}_call_segment_0.wav\n",
    "    #   â”‚   â”œâ”€â”€ {id}_{english_name}_{scientific_name}_{scientific_subspecies}_call_segment_1.wav\n",
    "    #   â”‚   â”œâ”€â”€ {id}_{english_name}_{scientific_name}_{scientific_subspecies}_call_segment_2.wav\n",
    "#   â”œâ”€â”€ etc.\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=segments_dir,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "# Set audio feature to not decode automatically to avoid torchcodec dependency\n",
    "dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mel_spectrogram(example):\n",
    "    audio_path = example[\"audio\"][\"path\"]\n",
    "    y, sr = librosa.load(audio_path, sr=44100) # 44.1 kHz is a common sampling rate for bird sounds\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mels=128, # Number of mel bands (more bands can capture more detail)\n",
    "        n_fft=2048, # FFT window size \n",
    "        hop_length=512, # Hop length for STFT\n",
    "        win_length=2048, # Window length for STFT\n",
    "        window='hann', # Window function\n",
    "        fmin=50, # Minimum frequency\n",
    "        fmax=sr//2 # Maximum frequency (Nyquist frequency for the given sample rate)\n",
    "    )\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=1.0) # Convert to dB scale\n",
    "    \n",
    "    # Convert to PIL Image instead of list\n",
    "    # Normalize the spectrogram to 0-255 range for image processing\n",
    "    normalized = ((mel_spectrogram_db - mel_spectrogram_db.min()) / \n",
    "                 (mel_spectrogram_db.max() - mel_spectrogram_db.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert to 3-channel image (RGB) by repeating the grayscale values\n",
    "    rgb_image = np.stack([normalized] * 3, axis=0)  # Shape: (3, height, width)\n",
    "    rgb_image = np.transpose(rgb_image, (1, 2, 0))  # Shape: (height, width, 3)\n",
    "    \n",
    "    example[\"image\"] = Image.fromarray(rgb_image)\n",
    "    return example\n",
    "\n",
    "def convert_to_linear_spectrogram(example):\n",
    "    audio_path = example[\"audio\"][\"path\"]\n",
    "    y, sr = librosa.load(audio_path, sr=44100)  # Load audio at 44.1 kHz\n",
    "    linear_spectrogram = librosa.stft(y, n_fft=2048, hop_length=512, win_length=2048)\n",
    "    linear_spectrogram_db = librosa.amplitude_to_db(np.abs(linear_spectrogram), ref=np.max)\n",
    "    \n",
    "    # Convert to PIL Image instead of list\n",
    "    normalized = ((linear_spectrogram_db - linear_spectrogram_db.min()) / \n",
    "                 (linear_spectrogram_db.max() - linear_spectrogram_db.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert to 3-channel image (RGB) by repeating the grayscale values\n",
    "    rgb_image = np.stack([normalized] * 3, axis=0)  # Shape: (3, height, width)\n",
    "    rgb_image = np.transpose(rgb_image, (1, 2, 0))  # Shape: (height, width, 3)\n",
    "    \n",
    "    example[\"image\"] = Image.fromarray(rgb_image)\n",
    "    return example\n",
    "\n",
    "def extract_bird_name_from_filename(example):\n",
    "    \"\"\"\n",
    "    Extract the English bird name from the filename.\n",
    "    Filename format: {file_id}_{english_name}_{scientific_specie}_{scientific_subspecie}_{song or call}.wav\n",
    "    We want the {english_name} part (second element after splitting by underscore).\n",
    "    \"\"\"\n",
    "    audio_path = example[\"audio\"][\"path\"]\n",
    "    # Get just the filename without the path\n",
    "    filename = os.path.basename(audio_path)\n",
    "    # Remove the .wav extension\n",
    "    filename_without_ext = os.path.splitext(filename)[0]\n",
    "    # Split by underscore and get the english name (second element)\n",
    "    parts = filename_without_ext.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        english_name = parts[1]  # Second element is the English name\n",
    "    else:\n",
    "        # Fallback if filename doesn't match expected format\n",
    "        english_name = \"unknown\"\n",
    "    \n",
    "    example[\"class\"] = english_name\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(\n",
    "    extract_bird_name_from_filename,\n",
    "    remove_columns=[\"label\"],\n",
    "    desc=\"Extracting bird names from filenames\"\n",
    ")\n",
    "\n",
    "# Apply mel spectrogram conversion to the dataset\n",
    "dataset = dataset.map(\n",
    "    convert_to_mel_spectrogram,\n",
    "    remove_columns=[\"audio\"],\n",
    "    desc=\"Converting to mel spectrograms\"\n",
    ")\n",
    "\n",
    "# dataset = dataset.map(\n",
    "#     convert_to_linear_spectrogram,\n",
    "#     remove_columns=[\"audio\"],\n",
    "#     desc=\"Converting to linear spectrograms\"\n",
    "# )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"test\": test,\n",
    "})\n",
    "\n",
    "# Access spectrogram path instead of decoded audio to avoid torchcodec\n",
    "print(f\"Number of training samples: {len(dataset_dict['train'])}\")\n",
    "print(f\"Number of test samples: {len(dataset_dict['test'])}\")\n",
    "print(f\"Sample spectrogram path: {dataset_dict['train'][10]['image'].filename}\")  # Show path to the spectrogram image\n",
    "print(f\"Sample class: {dataset_dict['train'][10]['class']}\")  # Show class label\n",
    "\n",
    "# Show the dataset structure\n",
    "print(f\"\\nDataset features: {dataset_dict['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406017e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mel spectrograms for visualization\n",
    "def plot_spectrogram(spectrogram, title=\"Mel Spectrogram\"):\n",
    "    # Convert PIL Image to numpy array and extract single channel for heatmap\n",
    "    if isinstance(spectrogram, Image.Image):\n",
    "        spectrogram_array = np.array(spectrogram)\n",
    "    else:\n",
    "        spectrogram_array = spectrogram\n",
    "    \n",
    "    # If it's RGB (3 channels), take only the first channel since all channels are identical\n",
    "    if len(spectrogram_array.shape) == 3:\n",
    "        spectrogram_2d = spectrogram_array[:, :, 0]  # Take first channel\n",
    "    else:\n",
    "        spectrogram_2d = spectrogram_array\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(spectrogram_2d, cmap='viridis', cbar=True)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Mel Frequency Bands')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot a few mel spectrograms from the training set\n",
    "for i in range(5):  # Show first 5 examples\n",
    "    spectrogram = dataset_dict['train'][i]['image']\n",
    "    bird_class = dataset_dict['train'][i]['class']\n",
    "    plot_spectrogram(spectrogram, title=f\"Mel Spectrogram - {bird_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b24ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all class labels without triggering repeated filtering\n",
    "temp_train = dataset_dict['train'].with_format(None)\n",
    "all_classes = [item['class'] for item in temp_train]\n",
    "class_counts = Counter(all_classes)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "# Sort by count (descending) for better visualisation\n",
    "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for bird_class, count in sorted_classes:\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"{bird_class:12}: {count:5,} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Total':12}: {total_samples:5,} (100.0%)\")\n",
    "print(f\"{'Classes':12}: {len(class_counts):5}\")\n",
    "\n",
    "# Check for class imbalance\n",
    "max_count = max(class_counts.values())\n",
    "min_count = min(class_counts.values())\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\nClass Balance Analysis:\")\n",
    "print(f\"    - Most samples: {max_count:,} ({sorted_classes[0][0]})\")\n",
    "print(f\"    - Least samples: {min_count:,} ({sorted_classes[-1][0]})\")\n",
    "print(f\"    - Imbalance ratio: {imbalance_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give user option on how many spectrograms per specie for training\n",
    "num_spectrograms_per_class = int(input(f\"Enter the number of spectrograms per class for training (min = {min_count}, max = {max_count}, 0 for all): \"))\n",
    "if num_spectrograms_per_class > 0:\n",
    "    # Group indices by class in a single pass\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, example in enumerate(dataset_dict['train']):\n",
    "        class_to_indices[example['class']].append(idx)\n",
    "    \n",
    "    selected_indices = []\n",
    "    for cls, indices in class_to_indices.items():\n",
    "        np.random.shuffle(indices)\n",
    "        selected_indices.extend(indices[:num_spectrograms_per_class])\n",
    "    \n",
    "    # Sort indices to optimise dataset selection\n",
    "    selected_indices = sorted(selected_indices)\n",
    "    \n",
    "    # Create filtered dataset\n",
    "    filtered_train = dataset_dict['train'].select(selected_indices)\n",
    "    dataset_dict['train'] = filtered_train\n",
    "else:\n",
    "    print(\"Using all spectrograms in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# pipeline for preprocessing images to get same shape and scaling\n",
    "preprocess_image = Compose([\n",
    "    Resize(image_processor.size[\"height\"]),\n",
    "    CenterCrop(image_processor.size[\"height\"]),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "# function to apply above preprocessing to a batch (train or test)\n",
    "def preprocess(batch):\n",
    "    batch[\"pixel_values\"] = [\n",
    "        preprocess_image(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "# Set the transform for the datasets\n",
    "dataset_dict[\"train\"].set_transform(preprocess)\n",
    "dataset_dict[\"test\"].set_transform(preprocess)\n",
    "\n",
    "print(f\"Training set size: {len(dataset_dict['train'])}, Test set size: {len(dataset_dict['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "\n",
    "    print(f\"Model Size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    params, trainable = 0, 0\n",
    "    \n",
    "    for _, p in model.named_parameters():\n",
    "        params += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{params:,} ({100 * trainable / params:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "'''\n",
    "# if you want to use quantized lora fine-tuning\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "'''\n",
    "\n",
    "def data_collator(batch):\n",
    "    \"\"\"\n",
    "    Custom data collator to handle batches of images and labels.\n",
    "    Converts pixel values to tensors and labels to a tensor.\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"    \n",
    "    Compute accuracy metrics for the evaluation predictions.\n",
    "    Extracts predictions and computes accuracy using the evaluate library.\n",
    "    \"\"\"\n",
    "    eval_pred = eval_pred\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def get_base_model(label2id, id2label):\n",
    "    \"\"\"    \n",
    "    Load the base model for image classification.\n",
    "    This function initializes the model with the specified label mappings.\n",
    "    \"\"\"\n",
    "    print(f\"Loading base model: {model_name}\")\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    #   quantization_config=bnb_config,\n",
    "    #   device_map=\"auto\",\n",
    "    )\n",
    "    # REMOVED: model.config.class_weights = class_weights_tensor\n",
    "    # This was causing JSON serialization error in TensorBoard\n",
    "    return model\n",
    "\n",
    "\n",
    "# Custom trainer class to handle class weights properly\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function with class weights for handling imbalanced data.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            # Move class weights to the same device as logits\n",
    "            device_weights = self.class_weights.to(logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=device_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def build_lora_model(label2id, id2label):\n",
    "    \"\"\"\n",
    "    Build the LoRA model for fine-tuning.\n",
    "    This function initializes the base model and applies LoRA configuration.\n",
    "    \"\"\"\n",
    "    print(\"Building LoRA model...\")\n",
    "    base_model = get_base_model(label2id, id2label)\n",
    "    print_trainable_parameters(base_model, label=\"Base Model\")\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16, # Rank of the LoRA layers\n",
    "        lora_alpha=16, # Scaling factor for the LoRA layers\n",
    "        target_modules=[\"query\", \"value\"], # Target modules for LoRA\n",
    "        lora_dropout=0.1, # Dropout rate for LoRA layers\n",
    "        bias=\"none\", # No bias in LoRA layers\n",
    "        modules_to_save=[\"classifier\"], # Modules to save during LoRA fine-tuning\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(base_model, config)\n",
    "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
    "\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17605f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_names = list(dataset_dict['train'].unique('class'))\n",
    "label2id = {name: i for i, name in enumerate(class_names)}\n",
    "id2label = {i: name for i, name in enumerate(class_names)}\n",
    "all_classes = [example['class'] for example in dataset_dict['train']]\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.array(range(len(class_names))),\n",
    "    y=[label2id[cls] for cls in all_classes]\n",
    ")\n",
    "\n",
    "# Convert to dictionary format for PyTorch\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"ðŸ“Š Calculated Class Weights:\")\n",
    "for class_name, class_id in label2id.items():\n",
    "    weight = class_weight_dict[class_id]\n",
    "    print(f\"{class_name:12}: {weight:.3f}\")\n",
    "\n",
    "# Add class weights to training arguments\n",
    "class_weights_tensor = torch.FloatTensor(list(class_weight_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./model-checkpoints\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4, # Learning rate for fine-tuning\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False, # Disable mixed precision training\n",
    "    bf16=False, # Disable bfloat16 training\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    label_names=[\"labels\"],\n",
    "    warmup_ratio=0.1, # Warmup ratio for learning rate scheduler\n",
    "    weight_decay=0.01, # Weight decay for regularization\n",
    "    dataloader_num_workers=0, # Number of workers for data loading\n",
    "    report_to=None,  # Disable TensorBoard\n",
    ")\n",
    "\n",
    "# Create train/validation split\n",
    "train_val_split = dataset_dict[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "test_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "temp_train = dataset_dict[\"train\"].with_format(None)\n",
    "unique_classes = sorted(set(temp_train[\"class\"]))\n",
    "label2id = {label: i for i, label in enumerate(unique_classes)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Found {len(unique_classes)} classes: {unique_classes}\")\n",
    "print(f\"Label mappings: {label2id}\")\n",
    "\n",
    "# Add early stopping to training arguments\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Number of epochs with no improvement to wait before stopping\n",
    "    early_stopping_threshold=0.01  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Create data collator as a class\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, label2id_mapping):\n",
    "        self.label2id_mapping = label2id_mapping\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Custom data collator to handle batches of images and labels.\n",
    "        Converts pixel values to tensors and labels to a tensor.\n",
    "        \"\"\"\n",
    "        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "        labels = torch.tensor([self.label2id_mapping[item[\"class\"]] for item in batch])\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "data_collator = CustomDataCollator(label2id)\n",
    "\n",
    "epochs = 10  # Number of epochs for training\n",
    "training_arguments.num_train_epochs = epochs\n",
    "processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Use WeightedTrainer instead of regular Trainer\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights_tensor,  # Pass class weights here\n",
    "    model=build_lora_model(label2id, id2label),\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],  # Add early stopping callback\n",
    "    tokenizer=processor,  # Use image processor for tokenization\n",
    ")\n",
    "\n",
    "print(f\"Class weights being used:\")\n",
    "for i, (class_name, weight) in enumerate(zip(unique_classes, class_weights_tensor.tolist())):\n",
    "    print(f\"  {class_name}: {weight:.3f}\")\n",
    "\n",
    "import tqdm\n",
    "print(f\"Starting training for {epochs} epochs with class weighting...\")\n",
    "results = tqdm.tqdm(trainer.train(), desc=\"Training Progress\", total=epochs)\n",
    "\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Training completed. Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "print_model_size(output_dir)\n",
    "print_trainable_parameters(trainer.model, label=\"Final Model\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Validation results: {val_results}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Save model and metrics\n",
    "trainer.save_model(output_dir)\n",
    "print_model_size(output_dir)\n",
    "print_trainable_parameters(trainer.model, label=\"Final Model\")\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "print(f\"Training completed in {epochs} epochs.\")\n",
    "print(f\"Final validation accuracy: {val_results['eval_accuracy']:.2f}\")\n",
    "print(f\"Final test accuracy: {test_results['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO REFERENCE\n",
    "\n",
    "# Doing the inference\n",
    "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
    "    base_model = get_base_model(label2id, id2label)\n",
    "    return PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "\n",
    "\n",
    "def predict_image_class(image, model, image_processor):\n",
    "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    class_index = logits.argmax(-1).item()\n",
    "    return model.config.id2label[class_index]\n",
    "\n",
    "model = build_inference_model(\n",
    "    dataset_dict['train']['label2id'],\n",
    "    dataset_dict['train']['id2label'],\n",
    "    output_dir\n",
    ")\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset2['train'][1234]['image']\n",
    "print(dataset2_id2label[dataset2['train'][1234]['class']])\n",
    "print(predict_image_class(img, model, processor))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(dataset, id2label, model, processor):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(dataset['test'])):\n",
    "        img = dataset['test'][i]['image']\n",
    "        label = dataset['test'][i]['label']\n",
    "        pred = predict(img, model, processor)\n",
    "        \n",
    "        if id2label[label] == pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            \n",
    "    return correct, incorrect, (correct / (len(dataset['test'])))\n",
    "\n",
    "correct, incorrect, accuracy = calculate_accuracy(dataset2, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lora-manuai.zip lora-manuai-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ManuAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
